{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a02d55",
   "metadata": {},
   "source": [
    "## DATA ENGINEERING PIPELINE - F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca00425",
   "metadata": {},
   "source": [
    "Overview: Build a production ready data engineering pipeline using python.\n",
    "\n",
    "Task: Build a pipeline to extract the data from F1 data folder and ensure you have performed basic exploration data analysis on the data to ensure the data is ready to be used for transformations.\n",
    "\n",
    "Within the same pipeline, build the following transformations, aggregations and data subsets:\n",
    "\n",
    " - What was the average time each driver spent at the pit stop for any given race?\n",
    " - Insert the missing code (e.g: ALO for Alonso) for all drivers\n",
    " - Select a season from data. Determine youngest andoldest at start and end of the season\n",
    "\n",
    "The pipeline should write to a CSV file following each transformation step.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Below outlines the steps to be performed:\n",
    "\n",
    "    01) Import the necessary modules for the project.\n",
    "    02) Define the functions that will faciliate the data engineering.\n",
    "    03) Transform the data in accordance with design requirements.\n",
    "    04) Load the transformed data to target destination in specified format.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa0d06",
   "metadata": {},
   "source": [
    "#### Import modules and settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91123f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import glob\n",
    "\n",
    "# Configure spark session\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"f1\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path = '/Users/macbook/Documents/Github/Data-Engineering/f1/data/'\n",
    "filenames = glob.glob(path + '/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e27ae",
   "metadata": {},
   "source": [
    "#### Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef326492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filenames):\n",
    "    for i in range(0,len(filenames)):\n",
    "        df = spark.read.format(\"csv\").options(header='true').load(filenames[i]).limit(10)\n",
    "        # Print dimensions of each file. \n",
    "        print(df.count(),len(df.columns))\n",
    "def extract():\n",
    "    pass\n",
    "\n",
    "def transform():\n",
    "    pass\n",
    "\n",
    "def load():\n",
    "    pass\n",
    "\n",
    "read(filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
