{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c55e025",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING ETL PIPELINE - XETRA DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d085730",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Onject Oriented Programming Example\n",
    "\n",
    "Aim:\n",
    "Write a production ready ETL pipeline using python and pandas.\n",
    "\n",
    "Overview:\n",
    "Xetra is a German stock exchange based in Frankfurt operated by Deutsche Börse Group. \n",
    "Data related to daily trading activity is stored publicly on the Amazon S3 database. \n",
    "(Update - as of July 2022 the data is no longer available. An archival S3 database will be used) \n",
    "\n",
    "Task:\n",
    "- Use jupyter notebook as a protoyping tool.\n",
    "- Request and extract source data from cloud based web services.\n",
    "- Use list comprehension to read and consolidate multiple source files.\n",
    "- Utilise pandas package tools to facilitate ETL process.\n",
    "- Design and stucture code using object oriented programming techniques. \n",
    "\n",
    "Below outlines the steps to be performed:\n",
    "    \n",
    "    1) Create adapter layer to handle access to API and web service infrastructure:\n",
    "        - Connect, read and write to external data sources.\n",
    "        - Use datetime filtering to specify range and exclude previously processed dates.\n",
    "        - Read and write to metafile to manage previously processed dates. \n",
    "       \n",
    "    2) Create application layer to handle ETL pipeline:\n",
    "        - Function to EXTRACT bucket data via list comprehension.\n",
    "        - Function to TRANSFORM bucket data using pandas functions. \n",
    "        - Function to LOAD transformed data to Amazon S3 target bucket. \n",
    "        \n",
    "    3) Create main() to serve as program entry point: \n",
    "        - Define user parameters and configurations.\n",
    "        - Run ETL application.\n",
    "        - Load transformed data into private S3 bucket.\n",
    "        - Read data from private S3 bucket to verify ETL pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1e289",
   "metadata": {},
   "source": [
    "# Define Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b66eb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages to be imported\n",
    "import boto3 #AWS service management package.\n",
    "import pandas as pd #Data analysis library.\n",
    "from io import StringIO #String buffer to read CSV files.\n",
    "from io import BytesIO #Bytes buffer to read PARQUET files.\n",
    "from datetime import datetime, timedelta #Facilitate calulations relating to day of trade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c54097",
   "metadata": {},
   "source": [
    "# Adapter Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "04eabf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to convert bucket data from csv into pandas dataframe.\n",
    "def read_csv_to_df (bucket, key, decoding = 'utf-8', sep =','):\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body') #Read data element from list.\n",
    "    csv_obj = csv_obj.read().decode('utf-8') #Store into to csv object in utf-8 format.\n",
    "    in_buf = StringIO(csv_obj) #Buffer to store csv object as string data.\n",
    "    df = pd.read_csv(in_buf, delimiter=sep) #Read data into pandas data frame.\n",
    "    return df\n",
    "\n",
    "#Method to write output df to target S3 bucket as a parquet file.\n",
    "def write_df_to_S3(bucket, df, key):\n",
    "    out_buf = BytesIO() #Buffer to store df object to parquet data. \n",
    "    df.to_parquet(out_buf, index=False) #Write df to parquet buffer.\n",
    "    bucket.put_object(Body=out_buf.getvalue(),Key=key) #Store data into S3 bucket\n",
    "    return True  \n",
    "\n",
    "#Method to write output df to target S3 bucket as a parquet file.\n",
    "def write_df_to_S3_csv(bucket, df, key):\n",
    "    out_buf = StringIO() #Buffer to store df object to parquet data. \n",
    "    df.to_csv(out_buf, index=False) #Write df to parquet buffer.\n",
    "    bucket.put_object(Body=out_buf.getvalue(),Key=key) #Store data into S3 bucket\n",
    "    return True  \n",
    "\n",
    "#Method to return list of files in bucket filtered by date. \n",
    "def list_files_in_prefix(bucket, prefix):\n",
    "    files = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48610e3",
   "metadata": {},
   "source": [
    "# Application Layer (Non-Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e4ca79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to return list of files in bucket between minimum filtering date and today's date.\n",
    "def return_date_list(bucket, arg_date, arg_date_format, meta_key): \n",
    "    today = datetime.today().date()\n",
    "    min_date = datetime.strptime(arg_date, arg_date_format)\n",
    "    min_date = min_date.date()- timedelta(days=1) #Previous calender day required for trade calculations.        \n",
    "    \n",
    "    #Read metafile that contains list of previously processed dates into dataframe. \n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key )\n",
    "        #Return list of dates between min_date and today_date).\n",
    "        dates = [(min_date + timedelta(days=x))for x in range (0 , (today - min_date).days + 1)]\n",
    "        #Store processed dates from metafile into a set. \n",
    "        meta_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        #Unique values in return_date_list when compared to meta_dates indicates dates to extract from S3 data.   \n",
    "        dates_to_extract = set(dates[1:]) - meta_dates\n",
    "        if dates_to_extract:\n",
    "            #Recalculate min_date.  \n",
    "            min_date = min(set(dates[1:]) - meta_dates)- timedelta(days=1)\n",
    "            #Filter return dates from source data based on verified min_date.\n",
    "            return_dates = [date.strftime(arg_date_format) for date in dates if date >= min_date]\n",
    "            return_min_date = (min_date + timedelta(days=1)).strftime(arg_date_format)\n",
    "        else: \n",
    "            return_dates = []\n",
    "            return_min_date = datetime(9999,1,1).date() \n",
    "    except bucket.session.client('s3').exceptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days=x)).strftime(arg_date_format)for x in range (0 , (today - min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "                            \n",
    "    return return_min_date,return_dates\n",
    "\n",
    "#Method to verify output by reading most recent target file back into workflow.\n",
    "def read_report(bucket_trg, trg_bucket_format):\n",
    "    \n",
    "    #Extract list of target bucket keys and filter most recent. \n",
    "    keys = [obj.key for obj in bucket_trg.objects.all() if obj.key.__contains__(trg_bucket_format)]\n",
    "    recent_key = keys[-1:][0]\n",
    "    \n",
    "    #Parse most recent file as a pandas dataframe report.     \n",
    "    prq_obj = bucket_trg.Object(key=recent_key).get().get('Body').read()\n",
    "    data = BytesIO(prq_obj)\n",
    "    df_report = pd.read_parquet(data)\n",
    "    return df_report\n",
    "\n",
    "#Method to update metafile that stores previously processed dates. \n",
    "def update_metafile(bucket, meta_key, extract_date_list):\n",
    "    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\n",
    "    df_new['source_date'] = extract_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    df_old = read_csv_to_df(bucket, meta_key)\n",
    "    df_all = pd.concat([df_old, df_new])\n",
    "    write_df_to_S3_csv(bucket, df_all, meta_key)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5b4b9",
   "metadata": {},
   "source": [
    "# Application Layer (Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "70a0f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to extract bucket data. \n",
    "def extract_bucket_data(bucket, date_list):\n",
    "    #Extract list of csv files from bucket by date prefix. \n",
    "    files = [key for date in date_list for key in list_files_in_prefix(bucket, date)]\n",
    "    #Read body of extracted csv files into master dataframe.\n",
    "    df = pd.concat([read_csv_to_df(bucket,obj) for obj in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#Method to transform bucket data. \n",
    "def transform_bucket_data(df,columns_use,arg_date):\n",
    "    #Method to reove unecessary columns and missing values from data.\n",
    "    df = df.loc[:,columns_use] #Remove unecessary columns \n",
    "    df.dropna(inplace=True) #Drop all missing values from the dataset.\n",
    "    df = df.reset_index(drop=True) #Reset the column index.\n",
    "    df.shape #Check if there was any filtering (should match table dimensions).\n",
    "    \n",
    "    #Get opening price per ISIN on a particular day.  \n",
    "    df['OpeningPrice'] = df.sort_values('Time').groupby(['ISIN','Date'])['StartPrice'].transform('first')\n",
    "\n",
    "    #Get closing price per ISIN on a particular day. \n",
    "    df['ClosingPrice'] = df.sort_values('Time').groupby(['ISIN','Date'])['EndPrice'].transform('last')\n",
    "\n",
    "    #Aggregate data per ISIN on a particular day.\n",
    "    df = df.groupby(['ISIN','Date'], as_index = False).agg(OpeningPriceEUR = ('OpeningPrice', 'min'),ClosingPriceEUR = ('ClosingPrice', 'min'), MinPriceEUR = ('MinPrice', 'min'), MaxPriceEUR = ('MaxPrice', 'max'), DailyTradedVolume = ('TradedVolume', 'sum'))\n",
    "\n",
    "    #Percentage change in closing price between current and pervious day of trade. \n",
    "    df['PrevClosingPriceEUR'] = df.sort_values(by = 'Date').groupby(['ISIN'])['ClosingPriceEUR'].shift(1)\n",
    "    df['DeltaPrevClosingPriceEUR%'] = (df['ClosingPriceEUR'] - df['PrevClosingPriceEUR'])/df['PrevClosingPriceEUR']*100\n",
    "\n",
    "    #Round aggregated data. \n",
    "    df = df.round(decimals = 2)\n",
    "\n",
    "    #Filter output by specified by argument date\n",
    "    df = df[df['Date']>=arg_date]\n",
    "    return df\n",
    "\n",
    "#Method to load transformed bucket data.\n",
    "def load_bucket_data(bucket, df, bucket_key, bucket_format, meta_key, extract_date_list):\n",
    "    #Parametised key name for Amazon target bucket. \n",
    "    bucket_key = bucket_key + datetime.today().strftime('%Y%m%d_%H%M%S') + bucket_format\n",
    "    write_df_to_S3(bucket, df, bucket_key)\n",
    "    update_metafile(bucket, meta_key, extract_date_list)\n",
    "    return True\n",
    "\n",
    "#ETL function. \n",
    "def etl_report(src_bucket, trg_bucket, date_list, columns_use, arg_date, trg_bucket_key, trg_bucket_format, meta_key):\n",
    "    extract_date_list = [date for date in date_list if date >= arg_date]\n",
    "    df = extract_bucket_data(src_bucket, date_list)\n",
    "    df = transform_bucket_data(df,columns_use,arg_date)\n",
    "    df = load_bucket_data(trg_bucket, df, trg_bucket_key, trg_bucket_format, meta_key, extract_date_list)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2206a",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "303eae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #User defined parameters and configurations. \n",
    "    arg_date = '2022-12-18' #Bucket filtering argument.\n",
    "    arg_date_format = '%Y-%m-%d' #Date format.\n",
    "    src_bucket = 'xetra-1234' #Source data bucket name\n",
    "    trg_bucket = 'xetra-probe'#Target data bucket name\n",
    "    trg_bucket_key = 'xetra_daily_report' #String to prepend target bucket name.\n",
    "    meta_key = 'meta_file.csv' #String name of metafile containing list of previously processed dates.\n",
    "    trg_bucket_format = '.parquet' #String to append target bucket name.\n",
    "    columns_use = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "\n",
    "    #Initialise connections and create bucket instances from Amazon S3 resource.\n",
    "    s3 = boto3.resource('s3') \n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "    \n",
    "    #Run ETL pipeline application.\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, arg_date_format, meta_key)\n",
    "    etl_report(bucket_src, bucket_trg, date_list, columns_use, extract_date, trg_bucket_key, trg_bucket_format, meta_key)\n",
    "    df_report =  read_report(bucket_trg, trg_bucket_format)    \n",
    "            \n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cb4660ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Date</th>\n",
       "      <th>OpeningPriceEUR</th>\n",
       "      <th>ClosingPriceEUR</th>\n",
       "      <th>MinPriceEUR</th>\n",
       "      <th>MaxPriceEUR</th>\n",
       "      <th>DailyTradedVolume</th>\n",
       "      <th>PrevClosingPriceEUR</th>\n",
       "      <th>DeltaPrevClosingPriceEUR%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-18</td>\n",
       "      <td>38.85</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.85</td>\n",
       "      <td>153</td>\n",
       "      <td>39.35</td>\n",
       "      <td>-1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>38.85</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.85</td>\n",
       "      <td>153</td>\n",
       "      <td>38.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>38.85</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.60</td>\n",
       "      <td>38.85</td>\n",
       "      <td>153</td>\n",
       "      <td>38.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>38.30</td>\n",
       "      <td>37.45</td>\n",
       "      <td>37.35</td>\n",
       "      <td>38.35</td>\n",
       "      <td>1179</td>\n",
       "      <td>38.60</td>\n",
       "      <td>-2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>36.40</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.40</td>\n",
       "      <td>36.85</td>\n",
       "      <td>1475</td>\n",
       "      <td>37.45</td>\n",
       "      <td>-1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19411</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19412</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19413</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>-3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19414</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0</td>\n",
       "      <td>3.43</td>\n",
       "      <td>-3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19415</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.43</td>\n",
       "      <td>24646</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19416 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISIN        Date  OpeningPriceEUR  ClosingPriceEUR  \\\n",
       "0      AT000000STR1  2022-12-18            38.85            38.60   \n",
       "1      AT000000STR1  2022-12-19            38.85            38.60   \n",
       "2      AT000000STR1  2022-12-20            38.85            38.60   \n",
       "3      AT000000STR1  2022-12-21            38.30            37.45   \n",
       "4      AT000000STR1  2022-12-22            36.40            36.85   \n",
       "...             ...         ...              ...              ...   \n",
       "19411  XS2434891219  2022-12-19             3.59             3.55   \n",
       "19412  XS2434891219  2022-12-20             3.59             3.55   \n",
       "19413  XS2434891219  2022-12-21             3.46             3.43   \n",
       "19414  XS2434891219  2022-12-22             3.26             3.32   \n",
       "19415  XS2434891219  2022-12-23             3.37             3.42   \n",
       "\n",
       "       MinPriceEUR  MaxPriceEUR  DailyTradedVolume  PrevClosingPriceEUR  \\\n",
       "0            38.60        38.85                153                39.35   \n",
       "1            38.60        38.85                153                38.60   \n",
       "2            38.60        38.85                153                38.60   \n",
       "3            37.35        38.35               1179                38.60   \n",
       "4            36.40        36.85               1475                37.45   \n",
       "...            ...          ...                ...                  ...   \n",
       "19411         3.55         3.59                  0                 3.55   \n",
       "19412         3.55         3.59                  0                 3.55   \n",
       "19413         3.32         3.46                  0                 3.55   \n",
       "19414         3.26         3.32                  0                 3.43   \n",
       "19415         3.37         3.43              24646                 3.32   \n",
       "\n",
       "       DeltaPrevClosingPriceEUR%  \n",
       "0                          -1.91  \n",
       "1                           0.00  \n",
       "2                           0.00  \n",
       "3                          -2.98  \n",
       "4                          -1.60  \n",
       "...                          ...  \n",
       "19411                       0.00  \n",
       "19412                       0.00  \n",
       "19413                      -3.41  \n",
       "19414                      -3.33  \n",
       "19415                       3.11  \n",
       "\n",
       "[19416 rows x 9 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run main function and display most recent target bucket file. \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
